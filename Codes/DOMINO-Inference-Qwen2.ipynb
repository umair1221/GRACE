{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [
          "setup"
        ]
      },
      "outputs": [],
      "source": [
        "\n",
        "# %pip -q install -U \"torch>=2.2\" \"torchvision>=0.17\" \"accelerate>=0.33\"     \"transformers>=4.46\" \"qwen-vl-utils>=0.0.8\"     \"pillow>=10\" \"pandas>=2.2\" \"numpy>=1.24\" \"tqdm>=4.66\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "75e24229",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/share_2/users/umair_nawaz/.conda/envs/qwen3-vl/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
            "  warnings.warn(\n",
            "/share_2/users/umair_nawaz/.conda/envs/qwen3-vl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from __future__ import annotations\n",
        "import os, re, json, glob\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageDraw\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "try:\n",
        "    from transformers import Qwen2VLForConditionalGeneration as Qwen2VLClass\n",
        "except Exception:\n",
        "    Qwen2VLClass = None\n",
        "\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "def natural_key(s: str):\n",
        "    base = os.path.basename(s)\n",
        "    return [int(t) if t.isdigit() else t.lower() for t in re.split(r\"(\\d+)\", base)]\n",
        "\n",
        "def ensure_dir(path: str):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def write_jsonl(path: str, rows):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in rows:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "tags": [
          "config"
        ]
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
        "\n",
        "# --- DOMINO paths ---\n",
        "DOMINO_ROOT  = \"/share_2/users/umair_nawaz/DOMINO-Evaluation/Medical/OHIF/SS\"\n",
        "PRED_DIR  = os.path.join(DOMINO_ROOT, \"Predictions-Qwen2-7B-VL\")\n",
        "\n",
        "# --- Model ---\n",
        "MODEL_ID   = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
        "DEVICE_MAP = \"cuda\"                          # nice default\n",
        "DTYPE      = torch.float16 if torch.cuda.is_available() else torch.bfloat16\n",
        "USE_FA2    = False\n",
        "MAX_NEW_TOKENS = 256\n",
        "\n",
        "# --- Evaluation ---\n",
        "PRIMARY_TAU   = 0.30\n",
        "THRESHOLDS    = [0.30, 0.50, 0.75, 0.90]\n",
        "REQUIRE_LABEL = True\n",
        "\n",
        "# --- Misc ---\n",
        "FILE_EXTS       = (\".png\", \".jpg\", \".jpeg\", \".PNG\", \".JPG\", \".JPEG\")\n",
        "SAVE_OVERLAYS   = True\n",
        "OUT_IMG_SUFFIX  = \"_pred.png\"\n",
        "OUTLINE_WIDTH   = 0\n",
        "LIMIT_PER_TASK  = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "imports_utils"
        ]
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "tags": [
          "json_extract"
        ]
      },
      "outputs": [],
      "source": [
        "\n",
        "def extract_json(text: str) -> Dict[str, Any]:\n",
        "    \"\"\"Extract the first top-level JSON object from text (brace-balanced).\"\"\"\n",
        "    m = re.search(r\"<tool_call>\\s*(\\{.*?\\})\\s*</tool_call>\", text, flags=re.S)\n",
        "    candidate = m.group(1) if m else None\n",
        "    if candidate is None:\n",
        "        depth, start = 0, None\n",
        "        for i, ch in enumerate(text):\n",
        "            if ch == \"{\":\n",
        "                if depth == 0:\n",
        "                    start = i\n",
        "                depth += 1\n",
        "            elif ch == \"}\":\n",
        "                if depth > 0:\n",
        "                    depth -= 1\n",
        "                    if depth == 0 and start is not None:\n",
        "                        candidate = text[start:i+1]\n",
        "                        break\n",
        "        if candidate is None:\n",
        "            raise ValueError(\"No JSON object found in model output.\")\n",
        "    s = re.sub(r\"<\\|.*?\\|>\", \"\", candidate).replace(\"\\n\", \" \")\n",
        "    return json.loads(s)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "tags": [
          "geom_draw"
        ]
      },
      "outputs": [],
      "source": [
        "\n",
        "GRID_MAX = 999\n",
        "_EPS = 1e-6\n",
        "\n",
        "def _grid999_to_px(v: float, size: int) -> int:\n",
        "    if size <= 0: return 0\n",
        "    px = int((float(v) / GRID_MAX) * size)\n",
        "    return max(0, min(size - 1, px))\n",
        "\n",
        "def _looks_norm01(vals) -> bool:\n",
        "    return max(vals) <= 1.5 + _EPS\n",
        "\n",
        "def _looks_grid999(vals) -> bool:\n",
        "    mx = max(vals)\n",
        "    return (mx <= GRID_MAX + _EPS) and (mx > 1.5 + _EPS)\n",
        "\n",
        "def clamp_box_to_img(x1, y1, x2, y2, W, H):\n",
        "    X1 = max(0, min(int(round(x1)), W - 1))\n",
        "    Y1 = max(0, min(int(round(y1)), H - 1))\n",
        "    X2 = max(0, min(int(round(x2)), W - 1))\n",
        "    Y2 = max(0, min(int(round(y2)), H - 1))\n",
        "    if X2 <= X1: X2 = min(W - 1, X1 + 2)\n",
        "    if Y2 <= Y1: Y2 = min(H - 1, Y1 + 2)\n",
        "    return (X1, Y1, X2, Y2)\n",
        "\n",
        "def iou_xyxy(a: Tuple[float,float,float,float], b: Tuple[float,float,float,float]) -> float:\n",
        "    ax1, ay1, ax2, ay2 = a\n",
        "    bx1, by1, bx2, by2 = b\n",
        "    inter_w = max(0.0, min(ax2, bx2) - max(ax1, bx1))\n",
        "    inter_h = max(0.0, min(ay2, by2) - max(ay1, by1))\n",
        "    inter   = inter_w * inter_h\n",
        "    area_a  = max(0.0, ax2 - ax1) * max(0.0, ay2 - ay1)\n",
        "    area_b  = max(0.0, bx2 - bx1) * max(0.0, by2 - by1)\n",
        "    denom   = area_a + area_b - inter\n",
        "    return (inter / denom) if denom > 0 else 0.0\n",
        "\n",
        "def draw_and_save(image_path: str, boxes_xyxy_px: List[Tuple[int,int,int,int]], labels: List[str], out_path: str, outline_width: int = 5):\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    d = ImageDraw.Draw(img)\n",
        "    labels = labels or [f\"box{i+1}\" for i in range(len(boxes_xyxy_px))]\n",
        "    for (x1, y1, x2, y2), lab in zip(boxes_xyxy_px, labels):\n",
        "        d.rectangle([x1, y1, x2, y2], outline=(255,0,0), width=outline_width)\n",
        "        try:\n",
        "            l,t,r,b = d.textbbox((0,0), lab); tw, th = r-l, b-t\n",
        "        except Exception:\n",
        "            tw, th = max(8*len(lab), 16), 16\n",
        "        pad = 4\n",
        "        chip = [x1, max(0, y1 - th - 2*pad), x1 + tw + 2*pad, y1]\n",
        "        d.rectangle(chip, fill=(255,0,0))\n",
        "        d.text((chip[0]+pad, chip[1]+pad), lab, fill=(255,255,255))\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    img.save(out_path, quality=95)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "tags": [
          "converter_fix"
        ]
      },
      "outputs": [],
      "source": [
        "\n",
        "def _disambiguate_to_ltrb(x1, y1, a, b, mode: str):\n",
        "    \"\"\"Return (X1,Y1,X2,Y2) interpreting (x1,y1,a,b) either as XYXY or XYWH.\n",
        "    mode in {\"norm\",\"grid\",\"pixel\"} controls scaling for XYWH->XYXY path.\n",
        "    We first try XYXY; if degenerate (x2<=x1 or y2<=y1), we treat as XYWH.\n",
        "    \"\"\"\n",
        "    if mode == \"norm\":\n",
        "        X1, Y1, X2, Y2 = x1, y1, a, b\n",
        "        if (X2 <= X1 + 1e-9) or (Y2 <= Y1 + 1e-9):\n",
        "            X2, Y2 = x1 + a, y1 + b\n",
        "        return X1, Y1, X2, Y2\n",
        "    elif mode == \"grid\":\n",
        "        X1, Y1, X2, Y2 = x1, y1, a, b\n",
        "        if (X2 <= X1 + 1e-6) or (Y2 <= Y1 + 1e-6):\n",
        "            X2, Y2 = x1 + a, y1 + b\n",
        "        return X1, Y1, X2, Y2\n",
        "    else:  # pixel\n",
        "        X1, Y1, X2, Y2 = x1, y1, a, b\n",
        "        if (X2 <= X1) or (Y2 <= Y1):\n",
        "            X2, Y2 = x1 + a, y1 + b\n",
        "        return X1, Y1, X2, Y2\n",
        "\n",
        "def boxes_px_from_res_json(res_json: dict, image_path: str):\n",
        "    \"\"\"Convert predicted boxes to absolute pixel XYXY with XYXY/XYWH auto-fix.\n",
        "    Default expectation for Qwen2-VL: xyxy_norm, but we robustly detect:\n",
        "      - xyxy_norm / xywh_norm\n",
        "      - xyxy_grid999 / xywh_grid999\n",
        "      - xyxy_pixel / xywh_pixel / xyxy / xywh\n",
        "    If format missing: auto-detect by value ranges (norm vs grid vs pixel),\n",
        "    then disambiguate XYXY vs XYWH by checking monotonicity.\n",
        "    \"\"\"\n",
        "    W, H = Image.open(image_path).size\n",
        "    fmt = (res_json.get(\"box_format\") or \"\").lower().strip()\n",
        "    out_boxes, out_labels = [], []\n",
        "\n",
        "    def clamp_and_scale(X1, Y1, X2, Y2, mode):\n",
        "        if mode == \"norm\":\n",
        "            X1, Y1, X2, Y2 = X1 * W, Y1 * H, X2 * W, Y2 * H\n",
        "        elif mode == \"grid\":\n",
        "            X1, Y1, X2, Y2 = _grid999_to_px(X1, W), _grid999_to_px(Y1, H), _grid999_to_px(X2, W), _grid999_to_px(Y2, H)\n",
        "        return clamp_box_to_img(X1, Y1, X2, Y2, W, H)\n",
        "\n",
        "    for item in res_json.get(\"click_boxes\", []):\n",
        "        lab = item.get(\"label\", \"box\")\n",
        "        box = item.get(\"box\", [])\n",
        "        if not (isinstance(box, (list, tuple)) and len(box) == 4):\n",
        "            continue\n",
        "        x1, y1, a, b = [float(v) for v in box]\n",
        "        vals = [abs(x1), abs(y1), abs(a), abs(b)]\n",
        "\n",
        "        if fmt in (\"xyxy_norm\",\"x1y1x2y2_norm\",\"xywh_norm\"):\n",
        "            mode = \"norm\"\n",
        "        elif fmt in (\"xyxy_grid999\",\"x1y1x2y2_grid999\",\"xywh_grid999\"):\n",
        "            mode = \"grid\"\n",
        "        elif fmt in (\"xyxy_pixel\",\"x1y1x2y2_pixel\",\"xywh_pixel\",\"xyxy\",\"x1y1x2y2\",\"xywh\"):\n",
        "            mode = \"pixel\"\n",
        "        else:\n",
        "            mode = \"norm\" if _looks_norm01(vals) else (\"grid\" if _looks_grid999(vals) else \"pixel\")\n",
        "\n",
        "        X1, Y1, X2, Y2 = _disambiguate_to_ltrb(x1, y1, a, b, mode)\n",
        "        px_box = clamp_and_scale(X1, Y1, X2, Y2, mode)\n",
        "\n",
        "        out_boxes.append(px_box)\n",
        "        out_labels.append(lab)\n",
        "\n",
        "    return out_boxes, out_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "tags": [
          "prompt_infer"
        ]
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_messages_for_image(image_path: str, task_query: str, image_instruction: str):\n",
        "    pil_img = Image.open(image_path).convert(\"RGB\")\n",
        "    sys_txt = (\n",
        "        \"You are a precise UI grounding assistant. \"\n",
        "        \"Given a GUI screenshot, a global task, and the step instruction for THIS image, \"\n",
        "        \"output STRICT JSON only with bounding boxes needed on THIS screen. \"\n",
        "        \"Use normalized coordinates in [0,1] relative to THIS image. \"\n",
        "        \"Prefer the format 'xyxy_norm' with [x1,y1,x2,y2]. \"\n",
        "        \"If you decide to use widths/heights, set 'box_format' to 'xywh_norm'. \"\n",
        "        \"Return a single JSON object and nothing else.\"\n",
        "    )\n",
        "    schema = (\n",
        "        \"{\\n\"\n",
        "        '  \"image\": \"<filename>\",\\n'\n",
        "        '  \"box_format\": \"xyxy_norm\",\\n'\n",
        "        '  \"click_boxes\": [\\n'\n",
        "        '     {\"label\": \"<instruction>\", \"box\": [x1,y1,x2,y2]}\\n'\n",
        "        \"  ]\\n\"\n",
        "        \"}\\n\"\n",
        "        \"Only valid JSON. Coordinates 0..1. Empty list if no action is needed.\"\n",
        "    )\n",
        "    system = {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": sys_txt}]}\n",
        "    user = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": f\"Global task: {task_query}\"},\n",
        "            {\"type\": \"text\", \"text\": f\"Instruction for this image: {image_instruction}\"},\n",
        "            {\"type\": \"text\", \"text\": f\"Image filename: {os.path.basename(image_path)}\"},\n",
        "            {\"type\": \"image\", \"image\": pil_img},\n",
        "            {\"type\": \"text\", \"text\": \"Return JSON with this schema:\"},\n",
        "            {\"type\": \"text\", \"text\": schema},\n",
        "        ],\n",
        "    }\n",
        "    return [system, user]\n",
        "\n",
        "@torch.inference_mode()\n",
        "def infer_one(model, processor, image_path: str, task_query: str, image_instruction: str,\n",
        "              temperature: float = 0.0, max_new_tokens: int = MAX_NEW_TOKENS):\n",
        "    messages = build_messages_for_image(image_path, task_query, image_instruction)\n",
        "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    images, videos = process_vision_info(messages)\n",
        "    inputs = processor(text=[text], images=images, videos=videos, return_tensors=\"pt\", padding=True)\n",
        "    for k, v in list(inputs.items()):\n",
        "        if hasattr(v, \"to\"):\n",
        "            inputs[k] = v.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    gen_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=(temperature is not None and temperature > 0),\n",
        "        temperature=(temperature if (temperature is not None and temperature > 0) else None),\n",
        "        top_p=None if (temperature is None or temperature == 0) else 0.95,\n",
        "    )\n",
        "    trimmed = gen_ids[:, inputs[\"input_ids\"].shape[1]:]\n",
        "    txt = processor.batch_decode(trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=True)[0]\n",
        "\n",
        "    try:\n",
        "        parsed = extract_json(txt)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] JSON parse failed for {os.path.basename(image_path)}: {e}\\nRAW[:400]: {txt[:400]}\")\n",
        "        parsed = {\"image\": os.path.basename(image_path), \"box_format\": \"xyxy_norm\", \"click_boxes\": []}\n",
        "\n",
        "    pj = json.loads(json.dumps(parsed))\n",
        "    for cb in pj.get(\"click_boxes\", []):\n",
        "        cb[\"label\"] = image_instruction\n",
        "\n",
        "    boxes_px, labels = boxes_px_from_res_json(pj, image_path)\n",
        "    W, H = Image.open(image_path).size\n",
        "    return {\n",
        "        \"raw_text\": txt,\n",
        "        \"prediction\": pj,\n",
        "        \"boxes_px\": boxes_px,\n",
        "        \"labels\": labels,\n",
        "        \"image_size\": (W, H),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "tags": [
          "model_loader"
        ]
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_qwen2vl(model_id: str = MODEL_ID, dtype=DTYPE, device_map=DEVICE_MAP, use_fa2: bool = USE_FA2):\n",
        "    kw = {\"trust_remote_code\": True}\n",
        "    if use_fa2:\n",
        "        kw[\"attn_implementation\"] = \"flash_attention_2\"\n",
        "        if dtype == \"auto\":\n",
        "            dtype = torch.bfloat16\n",
        "    if dtype != \"auto\":\n",
        "        kw[\"torch_dtype\"] = dtype\n",
        "    if device_map is not None:\n",
        "        kw[\"device_map\"] = device_map\n",
        "\n",
        "    if Qwen2VLClass is not None:\n",
        "        model = Qwen2VLClass.from_pretrained(model_id, **kw)\n",
        "    else:\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_id, **kw)\n",
        "    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "    eos = getattr(getattr(processor, \"tokenizer\", None), \"eos_token_id\", None)\n",
        "    if getattr(getattr(model, \"generation_config\", None), \"pad_token_id\", None) is None and eos is not None:\n",
        "        model.generation_config.pad_token_id = eos\n",
        "\n",
        "    model.eval()\n",
        "    return model, processor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "tags": [
          "domino_io"
        ]
      },
      "outputs": [],
      "source": [
        "\n",
        "def xywh_to_xyxy(b):\n",
        "    x, y, w, h = b\n",
        "    return (x, y, x + w, y + h)\n",
        "\n",
        "def load_annotations(task_dir: str) -> Dict[str, Any]:\n",
        "    annot_path = os.path.join(task_dir, \"annotations.json\")\n",
        "    with open(annot_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    by_img = {}\n",
        "    for im in data.get(\"images\", []):\n",
        "        W, H = im[\"width\"], im[\"height\"]\n",
        "        boxes = [xywh_to_xyxy(bb[\"bbox\"]) for bb in im.get(\"bboxes\", [])]\n",
        "        by_img[im[\"file_name\"]] = {\n",
        "            \"boxes\": boxes,\n",
        "            \"instruction\": im.get(\"instruction\", \"\"),\n",
        "            \"size\": (W, H),\n",
        "            \"step_index\": im.get(\"step_index\"),\n",
        "        }\n",
        "    task_query = data.get(\"task_query\") or data.get(\"task_name\") or os.path.basename(task_dir)\n",
        "    return {\"task_query\": task_query, \"by_image\": by_img}\n",
        "\n",
        "def is_task_dir(path: str) -> bool:\n",
        "    return os.path.isdir(path) and os.path.exists(os.path.join(path, \"annotations.json\"))\n",
        "\n",
        "def list_task_dirs(root_dir: str) -> List[str]:\n",
        "    return [\n",
        "        os.path.join(root_dir, name)\n",
        "        for name in sorted(os.listdir(root_dir), key=natural_key)\n",
        "        if is_task_dir(os.path.join(root_dir, name))\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "tags": [
          "predict_tasks"
        ]
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_all_tasks_domino(root_dir: str, pred_root: str, model, processor,\n",
        "                         temperature: float = 0.0, max_new_tokens: int = MAX_NEW_TOKENS, limit_per_task: int = 0):\n",
        "    ensure_dir(pred_root)\n",
        "    tasks = list_task_dirs(root_dir)\n",
        "    print(f\"Found {len(tasks)} task(s). Saving to: {pred_root}\")\n",
        "\n",
        "    agg_path = os.path.join(pred_root, \"all_predictions.jsonl\")\n",
        "    open(agg_path, \"w\", encoding=\"utf-8\").close()\n",
        "\n",
        "    for task_dir in tasks:\n",
        "        # load task meta\n",
        "        with open(os.path.join(task_dir, \"annotations.json\"), \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "        task_label = os.path.basename(task_dir)\n",
        "        task_query = data.get(\"task_query\") or data.get(\"task_name\") or task_label\n",
        "\n",
        "        out_task_dir = os.path.join(pred_root, task_label)\n",
        "        ensure_dir(out_task_dir)\n",
        "        pred_jsonl_path = os.path.join(out_task_dir, \"predictions.jsonl\")\n",
        "\n",
        "        # collect images\n",
        "        image_paths = []\n",
        "        for ext in FILE_EXTS:\n",
        "            image_paths += glob.glob(os.path.join(task_dir, f\"*{ext}\"))\n",
        "        image_paths = sorted(set(image_paths), key=natural_key)\n",
        "        if limit_per_task and limit_per_task > 0:\n",
        "            image_paths = image_paths[:limit_per_task]\n",
        "\n",
        "        # map filename -> instruction\n",
        "        instr_by_file = {im[\"file_name\"]: im.get(\"instruction\",\"\") for im in data.get(\"images\", [])}\n",
        "\n",
        "        with open(pred_jsonl_path, \"w\", encoding=\"utf-8\") as f_out, open(agg_path, \"a\", encoding=\"utf-8\") as agg_out:\n",
        "            for img_path in image_paths:\n",
        "                fname = os.path.basename(img_path)\n",
        "                instruction = instr_by_file.get(fname, \"\")\n",
        "\n",
        "                res = infer_one(model, processor, img_path, task_query, instruction,\n",
        "                                temperature=temperature, max_new_tokens=max_new_tokens)\n",
        "\n",
        "                boxes_px, labels = res[\"boxes_px\"], res[\"labels\"]\n",
        "                if SAVE_OVERLAYS:\n",
        "                    overlay_path = os.path.join(out_task_dir, f\"{os.path.splitext(fname)[0]}{OUT_IMG_SUFFIX}\")\n",
        "                    draw_and_save(img_path, boxes_px, labels, overlay_path, outline_width=OUTLINE_WIDTH)\n",
        "\n",
        "                record = {\n",
        "                    \"task\": task_label,\n",
        "                    \"task_query\": task_query,\n",
        "                    \"image\": fname,\n",
        "                    \"instruction\": instruction,\n",
        "                    \"image_relpath\": os.path.relpath(img_path, root_dir),\n",
        "                    \"image_size\": res[\"image_size\"],\n",
        "                    \"prediction\": res[\"prediction\"],\n",
        "                    \"boxes_px\": boxes_px,\n",
        "                    \"labels\": labels,\n",
        "                }\n",
        "                line = json.dumps(record, ensure_ascii=False)\n",
        "                f_out.write(line + \"\\n\"); agg_out.write(line + \"\\n\")\n",
        "                print(f\"  {task_label} :: {fname} -> boxes {len(boxes_px)}\")\n",
        "\n",
        "        print(f\"Saved: {pred_jsonl_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "tags": [
          "evaluation"
        ]
      },
      "outputs": [],
      "source": [
        "\n",
        "def greedy_match_iou(gt_boxes: List[Tuple[float,float,float,float]],\n",
        "                     pred_boxes: List[Tuple[int,int,int,int]]):\n",
        "    pairs = []\n",
        "    for gi, gb in enumerate(gt_boxes):\n",
        "        for pi, pb in enumerate(pred_boxes):\n",
        "            pairs.append((gi, pi, iou_xyxy(gb, pb)))\n",
        "    pairs.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    used_g, used_p, out = set(), set(), []\n",
        "    for gi, pi, i in pairs:\n",
        "        if gi in used_g or pi in used_p: continue\n",
        "        used_g.add(gi); used_p.add(pi)\n",
        "        out.append((gi, pi, i))\n",
        "    return out\n",
        "\n",
        "def boxes_px_from_prediction_record(rec: Dict[str, Any], root_dir: str) -> Tuple[List[Tuple[int,int,int,int]], List[str], Tuple[int,int]]:\n",
        "    if \"boxes_px\" in rec and isinstance(rec[\"boxes_px\"], list) and len(rec[\"boxes_px\"]) > 0:\n",
        "        boxes = [tuple(map(int, b)) for b in rec[\"boxes_px\"]]\n",
        "        labels = rec.get(\"labels\", [])\n",
        "        size = tuple(rec.get(\"image_size\", (0,0)))\n",
        "        return boxes, labels, size\n",
        "\n",
        "    pred = rec.get(\"prediction\") or {}\n",
        "    rel = rec.get(\"image_relpath\")\n",
        "    img_path = os.path.join(root_dir, rel) if rel else None\n",
        "    if img_path and os.path.exists(img_path):\n",
        "        boxes, labels = boxes_px_from_res_json(pred, img_path)\n",
        "        W, H = Image.open(img_path).size\n",
        "        return boxes, labels, (W, H)\n",
        "\n",
        "    # fallback if image missing\n",
        "    labels, boxes = [], []\n",
        "    for item in pred.get(\"click_boxes\", []):\n",
        "        lab = item.get(\"label\", \"box\"); box = item.get(\"box\", [])\n",
        "        if not (isinstance(box, (list,tuple)) and len(box) == 4): continue\n",
        "        x1, y1, a, b = [float(v) for v in box]\n",
        "        X1, Y1, X2, Y2 = x1, y1, a, b\n",
        "        if X2 <= X1 or Y2 <= Y1:\n",
        "            X2, Y2 = x1 + a, y1 + b\n",
        "        boxes.append((int(round(X1)), int(round(Y1)), int(round(X2)), int(round(Y2)))); labels.append(lab)\n",
        "    return boxes, labels, (0,0)\n",
        "\n",
        "def score_task(task_dir: str, pred_task_dir: str,\n",
        "               primary_tau: float = 0.5,\n",
        "               thresholds: List[float] = [0.25, 0.5, 0.75, 0.9],\n",
        "               require_label_match: bool = True) -> Dict[str, Any]:\n",
        "    gt_path = os.path.join(task_dir, \"annotations.json\")\n",
        "    if not os.path.exists(gt_path):\n",
        "        return {\"task\": os.path.basename(task_dir), \"exists\": False, \"reason\": \"missing annotations.json\"}\n",
        "    with open(gt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    gt_by_img = {}\n",
        "    for im in data.get(\"images\", []):\n",
        "        W, H = im[\"width\"], im[\"height\"]\n",
        "        boxes = [(x, y, x+w, y+h) for (x, y, w, h) in [bb[\"bbox\"] for bb in im.get(\"bboxes\", [])]]\n",
        "        gt_by_img[im[\"file_name\"]] = {\"boxes\": boxes, \"instruction\": im.get(\"instruction\", \"\"), \"size\": (W,H)}\n",
        "\n",
        "    pred_file = os.path.join(pred_task_dir, \"predictions.jsonl\")\n",
        "    if not os.path.exists(pred_file):\n",
        "        return {\"task\": os.path.basename(task_dir), \"exists\": False, \"reason\": \"missing predictions.jsonl\"}\n",
        "\n",
        "    preds = [json.loads(line) for line in open(pred_file, \"r\", encoding=\"utf-8\") if line.strip()]\n",
        "    preds_by_img = {p[\"image\"]: p for p in preds}\n",
        "\n",
        "    per_image_rows, overall_ious, step_pass = [], [], {}\n",
        "\n",
        "    for fname, meta in gt_by_img.items():\n",
        "        gt_boxes = meta[\"boxes\"]\n",
        "        instruction = meta[\"instruction\"]\n",
        "        prec = preds_by_img.get(fname)\n",
        "        if not prec:\n",
        "            per_image_rows.append({\"image\": fname, \"best_iou\": 0.0, \"gt_count\": len(gt_boxes), \"pred_count\": 0, \"note\":\"no_pred\"})\n",
        "            overall_ious.extend([0.0]*len(gt_boxes)); step_pass[fname] = False\n",
        "            continue\n",
        "\n",
        "        pred_boxes, pred_labels, _sz = boxes_px_from_prediction_record(prec, DOMINO_ROOT)\n",
        "        if require_label_match:\n",
        "            keep = [i for i, lab in enumerate(pred_labels) if str(lab).strip() == str(instruction).strip()]\n",
        "            pred_boxes = [pred_boxes[i] for i in keep]\n",
        "\n",
        "        matches = greedy_match_iou(gt_boxes, pred_boxes)\n",
        "        matched_iou = [0.0] * len(gt_boxes)\n",
        "        for gi, pi, i in matches:\n",
        "            matched_iou[gi] = max(matched_iou[gi], i)\n",
        "\n",
        "        step_success = all(i >= primary_tau for i in matched_iou) if gt_boxes else True\n",
        "        step_pass[fname] = step_success\n",
        "\n",
        "        per_image_rows.append({\n",
        "            \"image\": fname,\n",
        "            \"instruction\": instruction,\n",
        "            \"gt_count\": len(gt_boxes),\n",
        "            \"pred_count\": len(pred_boxes),\n",
        "            \"best_iou\": max(matched_iou) if matched_iou else 0.0,\n",
        "            f\"step_success@{primary_tau:.2f}\": bool(step_success),\n",
        "            \"ious_per_gt\": matched_iou,\n",
        "        })\n",
        "        overall_ious.extend(matched_iou if matched_iou else [0.0])\n",
        "\n",
        "    task_completed = all(step_pass.values()) if step_pass else False\n",
        "    mIoU = (sum(overall_ious) / max(1, len(overall_ious)))\n",
        "    acc_at = {f\"Acc@{t:.1f}\": (sum(1 for v in overall_ious if v >= t) / max(1, len(overall_ious))) for t in thresholds}\n",
        "\n",
        "    return {\n",
        "        \"task\": os.path.basename(task_dir),\n",
        "        \"exists\": True,\n",
        "        f\"task_completed@{primary_tau:.2f}\": bool(task_completed),\n",
        "        \"num_images\": len(gt_by_img),\n",
        "        \"num_gt_boxes\": int(sum(len(v[\"boxes\"]) for v in gt_by_img.values())),\n",
        "        \"mIoU\": mIoU,\n",
        "        **acc_at,\n",
        "        \"per_image\": per_image_rows,\n",
        "    }\n",
        "\n",
        "def evaluate_all_domino(root_dir: str, pred_dir: str,\n",
        "                        primary_tau: float, thresholds: List[float], require_label_match: bool = True):\n",
        "    tasks = list_task_dirs(root_dir)\n",
        "    print(f\"Found {len(tasks)} task(s) with annotations.\")\n",
        "\n",
        "    per_task, overall_ious, completed = [], [], 0\n",
        "    for task_dir in tasks:\n",
        "        task_name = os.path.basename(task_dir)\n",
        "        pred_task_dir = os.path.join(pred_dir, task_name)\n",
        "        res = score_task(task_dir, pred_task_dir, primary_tau=primary_tau, thresholds=thresholds, require_label_match=require_label_match)\n",
        "        per_task.append(res)\n",
        "\n",
        "        if res.get(\"exists\"):\n",
        "            for row in res.get(\"per_image\", []):\n",
        "                overall_ious.extend(row.get(\"ious_per_gt\", []))\n",
        "            if res.get(f\"task_completed@{primary_tau:.2f}\"): completed += 1\n",
        "\n",
        "        out_task_json = os.path.join(pred_dir, task_name, \"eval.json\")\n",
        "        with open(out_task_json, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(res, f, indent=2)\n",
        "        print(f\"  Saved per-task eval: {out_task_json}\")\n",
        "\n",
        "    n_tasks = len(tasks)\n",
        "    overall_gt = len(overall_ious)\n",
        "    overall_mIoU = (sum(overall_ious) / overall_gt) if overall_gt else 0.0\n",
        "    overall_acc = {f\"Acc@{t:.1f}\": (sum(1 for v in overall_ious if v >= t) / overall_gt) if overall_gt else 0.0 for t in thresholds}\n",
        "    task_completion_rate = (completed / n_tasks) if n_tasks else 0.0\n",
        "\n",
        "    overall = {\n",
        "        \"num_tasks\": n_tasks,\n",
        "        \"overall_num_gt_boxes\": int(overall_gt),\n",
        "        f\"task_completion_rate@{primary_tau:.2f}\": task_completion_rate,\n",
        "        \"overall_mIoU\": overall_mIoU,\n",
        "        **overall_acc,\n",
        "    }\n",
        "\n",
        "    out_summary = os.path.join(pred_dir, \"_eval_summary.json\")\n",
        "    with open(out_summary, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"overall\": overall, \"per_task\": per_task}, f, indent=2)\n",
        "    print(\"\\n=== Overall Summary ===\")\n",
        "    for k, v in overall.items():\n",
        "        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
        "    print(f\"\\nSaved: {out_summary}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "run"
        ]
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load model\n",
        "def load_model_and_processor():\n",
        "    kw = {\"trust_remote_code\": True}\n",
        "    if USE_FA2:\n",
        "        kw[\"attn_implementation\"] = \"flash_attention_2\"\n",
        "        if DTYPE == \"auto\":\n",
        "            kw[\"torch_dtype\"] = torch.bfloat16\n",
        "    if DTYPE != \"auto\":\n",
        "        kw[\"torch_dtype\"] = DTYPE\n",
        "    if DEVICE_MAP is not None:\n",
        "        kw[\"device_map\"] = DEVICE_MAP\n",
        "\n",
        "    if Qwen2VLClass is not None:\n",
        "        model = Qwen2VLClass.from_pretrained(MODEL_ID, **kw)\n",
        "    else:\n",
        "        model = AutoModelForCausalLM.from_pretrained(MODEL_ID, **kw)\n",
        "    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "\n",
        "    eos = getattr(getattr(processor, \"tokenizer\", None), \"eos_token_id\", None)\n",
        "    if getattr(getattr(model, \"generation_config\", None), \"pad_token_id\", None) is None and eos is not None:\n",
        "        model.generation_config.pad_token_id = eos\n",
        "\n",
        "    model.eval()\n",
        "    return model, processor\n",
        "\n",
        "# model, processor = load_model_and_processor()\n",
        "# print(\"Model ready.\")\n",
        "\n",
        "# 1) Predict across tasks\n",
        "run_all_tasks_domino(\n",
        "    DOMINO_ROOT, PRED_DIR, model, processor,\n",
        "    temperature=0.0, max_new_tokens=MAX_NEW_TOKENS, limit_per_task=LIMIT_PER_TASK\n",
        ")\n",
        "\n",
        "# 2) Evaluate\n",
        "evaluate_all_domino(\n",
        "    DOMINO_ROOT, PRED_DIR,\n",
        "    primary_tau=PRIMARY_TAU,\n",
        "    thresholds=THRESHOLDS,\n",
        "    require_label_match=REQUIRE_LABEL\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95ce8a57",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f582910",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9509eeb3",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "qwen3-vl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
